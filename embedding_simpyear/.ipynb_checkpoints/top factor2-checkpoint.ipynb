{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline \n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "#有中文出现的情况，需要u'内容'\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "import scipy.stats as stats\n",
    "from datum import *\n",
    "import model\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_list = []\n",
    "test_list = []\n",
    "base_vali_list = []\n",
    "base_test_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(top_num, vali, test):\n",
    "    vali_abs = np.abs(vali)\n",
    "    arg_sort = np.argsort(vali_abs)\n",
    "    select_arg = arg_sort[-top_num:]\n",
    "    print('minimum: {}'.format(vali[select_arg[0]]))\n",
    "    select_vali = vali[select_arg]\n",
    "    select_test = test[select_arg]\n",
    "    true_test = np.abs(select_test)\n",
    "    #true_test = np.nan_to_num(select_test * select_vali / np.abs(select_vali))\n",
    "    print('mean: {}'.format(true_test.mean()))\n",
    "    return true_test, select_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    data = Datum('{}_{}_{}'.format(2005010120150101+i, 2015010120160101, 'year'))\n",
    "    data.data_prepare()\n",
    "    data.get_embedding()\n",
    "    data.supervised_data_prepare()\n",
    "    data.ic_prepare()\n",
    "\n",
    "    reload(model)\n",
    "    tf.reset_default_graph()\n",
    "    mod = model.Model(1, '{}_{}_{}'.format(2005010120150101+i, 2015010120160101, 'year'))\n",
    "    mod.data_initial(data)\n",
    "    mod.data_split()\n",
    "    mod.factor_network() \n",
    "    \n",
    "    for j in range(3):\n",
    "        model_num = j\n",
    "        saver = tf.train.Saver(max_to_keep=50)\n",
    "        data_dir = '/data/zhige_data/embedding_simpyear/'\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, data_dir+'{}model_{}/logmodel.ckpt'.format(model_num, mod.param))\n",
    "            rank_ic = np.zeros((mod.vali_test-mod.train_vali, 44, 4))\n",
    "            embedding = mod.data.embedding[:len(mod.data.list_stocks), :]\n",
    "            for count_day, day in enumerate(range(mod.train_vali,mod.vali_test)):#batch_num\n",
    "                for fac_idx in range(44):\n",
    "                    feature = mod.data.feature_data[:, day, fac_idx]\n",
    "                    if feature.std() != 0:\n",
    "                        feature = (feature-feature.min()) / (feature.max() - feature.min())\n",
    "                    else:\n",
    "                        continue\n",
    "                    label = mod.data.ar_ic[:, day, 2]\n",
    "                    feed_dict = {mod.embedding: embedding, mod.factor: feature, mod.factor_index: [fac_idx], mod.ic: label}\n",
    "                    new_f, loss_val = sess.run([mod.new_f, mod.cost], feed_dict=feed_dict)\n",
    "                    for ic_idx in range(4):\n",
    "                        rank_ic[count_day, fac_idx, ic_idx] = stats.spearmanr(mod.data.ar_ic[:, day, ic_idx], new_f)[0]\n",
    "            vali_ic_mean = rank_ic.mean(axis=0)[:, 2]\n",
    "            vali_list.append(vali_ic_mean)\n",
    "\n",
    "            rank_ic = np.zeros((mod.day_sample-mod.vali_test, 44, 4))\n",
    "            embedding = mod.data.embedding[:len(mod.data.list_stocks), :]\n",
    "            print('test: model:{}'.format(model_num))\n",
    "            for count_day, day in enumerate(range(mod.vali_test, mod.day_sample)):#batch_num\n",
    "                for fac_idx in range(44):\n",
    "                    feature = mod.data.feature_data[:, day, fac_idx]\n",
    "                    if feature.std() != 0:\n",
    "                        feature = (feature-feature.min()) / (feature.max() - feature.min())\n",
    "                    else:\n",
    "                        continue\n",
    "                    label = mod.data.ar_ic[:, day, 2]\n",
    "                    feed_dict = {mod.embedding: embedding, mod.factor: feature, mod.factor_index: [fac_idx], mod.ic: label}\n",
    "                    new_f, loss_val = sess.run([mod.new_f, mod.cost], feed_dict=feed_dict)\n",
    "                    for ic_idx in range(4):\n",
    "                        rank_ic[count_day, fac_idx, ic_idx] = stats.spearmanr(mod.data.ar_ic[:, day, ic_idx], new_f)[0]\n",
    "            avg_ic = rank_ic.mean(axis=0)[:, 2]\n",
    "            test_list.append(avg_ic)\n",
    "            print(np.abs(avg_ic))\n",
    "            print(np.abs(avg_ic).mean())         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_ic = np.zeros((mod.vali_test-mod.train_vali, 44, 4))\n",
    "for count_day, day in enumerate(range(mod.train_vali,mod.vali_test)):#batch_num\n",
    "    for fac_idx in range(44):\n",
    "        feature = mod.data.feature_data[:, day, fac_idx]\n",
    "        for ic_idx in range(4):\n",
    "            rank_ic[count_day, fac_idx, ic_idx] = np.nan_to_num(stats.spearmanr(mod.data.ar_ic[:, day, ic_idx], feature)[0])\n",
    "vali_ic_mean = rank_ic.mean(axis=0)[:, 2]\n",
    "base_vali_list.append(vali_ic_mean)           \n",
    "\n",
    "rank_ic = np.zeros((mod.day_sample-mod.vali_test, 44, 4))\n",
    "for count_day, day in enumerate(range(mod.vali_test, mod.day_sample)):#batch_num\n",
    "    for fac_idx in range(44):\n",
    "        feature = mod.data.feature_data[:, day, fac_idx]\n",
    "        for ic_idx in range(4):\n",
    "            rank_ic[count_day, fac_idx, ic_idx] = np.nan_to_num(stats.spearmanr(mod.data.ar_ic[:, day, ic_idx], feature)[0])\n",
    "avg_ic = rank_ic.mean(axis=0)[:, 2]\n",
    "base_test_list.append(avg_ic)\n",
    "print(np.abs(avg_ic))\n",
    "print(np.abs(avg_ic).mean())             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum: -0.0618400621218\n",
      "mean: 0.0329423803682\n",
      "minimum: -0.0558313087437\n",
      "mean: 0.027386178848\n",
      "minimum: 0.0567221046828\n",
      "mean: 0.0253573208902\n",
      "minimum: -0.0596140845854\n",
      "mean: 0.0273883355039\n",
      "minimum: -0.0660018916833\n",
      "mean: 0.0301081214131\n",
      "minimum: -0.0603054503604\n",
      "mean: 0.0281924767172\n",
      "minimum: -0.060734077485\n",
      "mean: 0.0267108515366\n",
      "minimum: -0.0599099010413\n",
      "mean: 0.0254367924006\n",
      "minimum: -0.0600357382565\n",
      "mean: 0.0285551386712\n",
      "0.0280086218166\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "for i in range(9):\n",
    "    select_fac, select_arg = select(10, vali_list[i], test_list[i])\n",
    "    mean += select_fac.mean()\n",
    "print(mean / 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum: -0.123104440843\n",
      "mean: 0.0223935175723\n"
     ]
    }
   ],
   "source": [
    "select_fac, select_arg = select(10, base_vali_list[-1], base_test_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.abs(np.array(test_list)).mean(axis=0)).to_csv(data_dir+'test_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.abs(np.array(base_test_list)).mean(axis=0)).to_csv(data_dir+'baseline_test_2015.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /data/zhige_data/huaxia_embedding/0model_2005010120150101_2015010120160101_year/logmodel.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /data/zhige_data/huaxia_embedding/1model_2005010120150101_2015010120160101_year/logmodel.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /data/zhige_data/huaxia_embedding/2model_2005010120150101_2015010120160101_year/logmodel.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /data/zhige_data/huaxia_embedding/0model_2005010120150102_2015010120160101_year/logmodel.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /data/zhige_data/huaxia_embedding/1model_2005010120150102_2015010120160101_year/logmodel.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /data/zhige_data/huaxia_embedding/2model_2005010120150102_2015010120160101_year/logmodel.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /data/zhige_data/huaxia_embedding/0model_2005010120150103_2015010120160101_year/logmodel.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /data/zhige_data/huaxia_embedding/1model_2005010120150103_2015010120160101_year/logmodel.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /data/zhige_data/huaxia_embedding/2model_2005010120150103_2015010120160101_year/logmodel.ckpt\n"
     ]
    }
   ],
   "source": [
    "list_confidence = []\n",
    "for i in range(3):\n",
    "    data = Datum('{}_{}_{}'.format(2005010120150101+i, 2015010120160101, 'year'))\n",
    "    data.data_prepare()\n",
    "    data.get_embedding()\n",
    "\n",
    "    reload(model)\n",
    "    tf.reset_default_graph()\n",
    "    mod = model.Model(1, '{}_{}_{}'.format(2005010120150101+i, 2015010120160101, 'year'))\n",
    "    mod.data_initial(data)\n",
    "    mod.factor_network() \n",
    "    \n",
    "    for j in range(3):\n",
    "        model_num = j\n",
    "        saver = tf.train.Saver(max_to_keep=50)\n",
    "        data_dir = '/data/zhige_data/huaxia_embedding/'\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, data_dir+'{}model_{}/logmodel.ckpt'.format(model_num, mod.param))\n",
    "            embedding = mod.data.embedding[:len(mod.data.list_stocks), :]\n",
    "            hold_place = np.zeros(len(mod.data.list_stocks))\n",
    "            feed_dict = {mod.embedding: embedding, mod.factor: hold_place, mod.factor_index: [0], mod.ic: hold_place}\n",
    "            u_bias = sess.run(mod.u_bias, feed_dict=feed_dict)\n",
    "        hidden = np.matmul(embedding, u_bias.T)\n",
    "        confidence = np.exp(hidden) / np.sum(np.exp(hidden), axis=0)\n",
    "        list_confidence.append(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "mod_list = []\n",
    "for i in range(3):\n",
    "    data = Datum('{}_{}_{}'.format(2005010120150101+i, 2015010120160101, 'year'))\n",
    "    data.data_prepare()\n",
    "    data.get_embedding()\n",
    "    data.supervised_data_prepare()\n",
    "    data.ic_prepare()\n",
    "    data_list.append(data)\n",
    "\n",
    "\n",
    "    mod = model.Model(1, '{}_{}_{}'.format(2005010120150101+i, 2015010120160101, 'year'))\n",
    "    mod.data_initial(data)\n",
    "    mod.data_split()\n",
    "    mod_list.append(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhongzheng500 = list(pd.read_csv('/data/zhige_data/zhongzheng500.csv', header=None, dtype=str)[0])\n",
    "use_index = [i for i in range(len(data.list_stocks)) if data.list_stocks[i] in zhongzheng500]\n",
    "next_return = data.price_data - data.price_data.mean(axis=0)\n",
    "next_return = next_return[np.array(use_index)]\n",
    "raw_feature = data.feature_data[np.array(use_index)]\n",
    "new_feature = np.zeros(data.feature_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "emb_num = 0\n",
    "model_num = 0\n",
    "mod = mod_list[emb_num]\n",
    "mod.factor_network() \n",
    "saver = tf.train.Saver(max_to_keep=50)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, data_dir+'{}model_{}/logmodel_{}.ckpt'.format(model_num, mod.param, model_num_list[3*emb_num+model_num]))\n",
    "    embedding = mod.data.embedding[:len(mod.data.list_stocks), :]\n",
    "    for day in range(mod.train_vali,mod.vali_test):\n",
    "        for fac_idx in range(44):\n",
    "            feature = mod.data.feature_data[:, day, fac_idx]\n",
    "            if feature.std() != 0:\n",
    "                feature = (feature-feature.min()) / (feature.max() - feature.min())\n",
    "            else:\n",
    "                continue\n",
    "            label = mod.data.ar_ic[:, day, 2]\n",
    "            feed_dict = {mod.embedding: embedding, mod.factor: feature, mod.factor_index: [fac_idx], mod.ic: label}\n",
    "            new_f, loss_val = sess.run([mod.new_f, mod.cost], feed_dict=feed_dict)\n",
    "            new_feature[:, day, fac_idx] = new_f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
