{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from 'model.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline \n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "#有中文出现的情况，需要u'内容'\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from datum import *\n",
    "import model\n",
    "import imp\n",
    "\n",
    "imp.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Datum()\n",
    "data.data_prepare()\n",
    "data.get_embedding('embedding_all.emb')\n",
    "data.supervised_data_prepare()\n",
    "data.evaluation_prepare()\n",
    "data.label_trend()\n",
    "data.label_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import linear_model as LR\n",
    "from sklearn import metrics as mt\n",
    "import scipy.stats as stats\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from six.moves import xrange\n",
    "\n",
    "import datum\n",
    "\n",
    "data_index = 0\n",
    "len_stock = 3145\n",
    "len_fund  = 2199\n",
    "\n",
    "def stock2all(a):\n",
    "    b = a.swapaxes(0, 2)\n",
    "    c = b.reshape(b.shape[0], -1)\n",
    "    d = c.T\n",
    "    return d\n",
    "\n",
    "a = np.array(range(27)).reshape((3, 3, 3))\n",
    "b = stock2all(a)\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, learning_rate_rank):\n",
    "        self.save_rank = learning_rate_rank\n",
    "        self.learning_rate = 1 / np.power(10, learning_rate_rank)\n",
    "        self.batch_size = 10\n",
    "\n",
    "    def data_initial(self, datum = None):\n",
    "        if datum is not None:\n",
    "            self.data = datum\n",
    "        else:\n",
    "            self.data = datum.Datum()\n",
    "            self.data.data_prepare()\n",
    "            self.data.evaluation_prepare()\n",
    "            self.data.label_trend()\n",
    "            self.data.label_return()\n",
    "        \n",
    "    def data_split(self):\n",
    "        self.day_sample = self.data.price_data.shape[1]\n",
    "        self.stock_sample = self.data.price_data.shape[0]\n",
    "        \n",
    "        self.train_vali = self.day_sample // 2\n",
    "        self.vali_test = self.train_vali + self.day_sample // 4\n",
    "        \n",
    "        self.rank_day = np.array(range(self.train_vali))\n",
    "        self.rank_stock = np.array(range(self.stock_sample))\n",
    "\n",
    "        use_index = []\n",
    "        for stock_idx in range(self.stock_sample):\n",
    "            stock_name = str(self.data.code_tag[stock_idx])\n",
    "            for _ in range(6-len(stock_name)):\n",
    "                stock_name = '0' + stock_name\n",
    "            if stock_name not in self.data.list_stocks:\n",
    "                continue\n",
    "            else:\n",
    "                use_index.append(stock_idx)\n",
    "        self.use_index = np.array(use_index)        \n",
    "        \n",
    "    def factor_network(self):\n",
    "        learning_rate = self.learning_rate\n",
    "        \n",
    "        self.embedding = tf.placeholder(tf.float32, shape=[32, 1], name='embedding')\n",
    "        self.factor = tf.placeholder(tf.float32, shape=[44, None], name='factor')\n",
    "        self.factor_index = tf.placeholder(tf.int32, shape=[5], name='factor_index')\n",
    "        self.label = tf.placeholder(tf.int32, shape=[None, 2], name='lable')\n",
    "        \n",
    "        self.u_bias = tf.get_variable('u_bias', shape=[44, 1, 32], initializer=tf.truncated_normal_initializer(stddev=1.0))\n",
    "        self.weight = tf.get_variable(name='weight', shape=[32, 32], initializer=tf.truncated_normal_initializer(stddev=1.0))\n",
    "        self.bias = tf.get_variable(name='bias', shape=[32], initializer=tf.zeros_initializer)\n",
    "        self.project = tf.get_variable(name='project', shape=[32], initializer=tf.truncated_normal_initializer(stddev=1.0))\n",
    "        \n",
    "        self.u_bias_select = tf.nn.embedding_lookup(self.u_bias, self.factor_index)\n",
    "        self.factor_select = tf.nn.embedding_lookup(self.factor, self.factor_index)\n",
    "        \n",
    "        concat = []\n",
    "        \n",
    "        for i in range(5):\n",
    "            add1 = tf.transpose(tf.matmul(self.weight, self.embedding))\n",
    "            add2 = tf.matmul(tf.expand_dims(self.factor_select[i, :], 1), self.u_bias_select[i])\n",
    "            concat.append(tf.reduce_sum(self.project * tf.tanh(add1 + add2 + self.bias), axis=1, keep_dims=True))\n",
    "        self.hidden = tf.concat([concat[i] for i in range(5)], axis=1)\n",
    "        \n",
    "\n",
    "        all_feature = tf.concat([tf.transpose(self.factor_select), self.hidden], 1)\n",
    "        self.pred = tf.contrib.layers.fully_connected(\n",
    "            inputs=all_feature,\n",
    "            num_outputs=2,  # hidden\n",
    "            activation_fn=tf.tanh,\n",
    "            weights_initializer=tf.truncated_normal_initializer(stddev=1.0),\n",
    "            biases_initializer=tf.zeros_initializer()\n",
    "        )\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.pred, labels=self.label))\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.cost)\n",
    "        \n",
    "        concat_all = []\n",
    "        \n",
    "        for i in range(44):\n",
    "            add3 = tf.transpose(tf.matmul(self.weight, self.embedding))\n",
    "            add4 = tf.matmul(tf.expand_dims(self.factor[i, :], 1), self.u_bias[i])\n",
    "            concat_all.append(tf.reduce_sum(self.project * tf.tanh(add3 + add4 + self.bias), axis=1, keep_dims=True))\n",
    "        self.evaluation = tf.concat([concat_all[i] for i in range(44)], axis=1)\n",
    "        \n",
    "    def training(self):\n",
    "        epochs = 50\n",
    "        batch_num = self.train_vali // self.batch_size\n",
    "        saver = tf.train.Saver(max_to_keep=None)\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, 'model_initial2/logmodel.ckpt')\n",
    "            for epoch in range(epochs):\n",
    "                print('epoch: {}'.format(epoch))\n",
    "                # validation\n",
    "                new_f_total = []\n",
    "                for stock_idx in range(self.use_index):\n",
    "                    price = self.data.price_data[stock_idx, self.train_vali:self.vali_test]\n",
    "                    feature = self.data.feature_data[stock_idx, self.train_vali:self.vali_test, :]\n",
    "                    label = self.data.ar_trend[stock_idx, self.train_vali:self.vali_test, :]\n",
    "                    stock_name = str(self.data.code_tag[stock_idx])\n",
    "                    embed = np.expand_dims(self.data.embedding[self.data.list_stocks.index(stock_name)], axis=1)\n",
    "                    factor_index = random.sample(list(range(44)), 5)\n",
    "                    feed_dict = {self.embedding: embed, self.factor: feature.T, self.factor_index: factor_index, self.label: label}\n",
    "                    new_f= sess.run(self.evaluation, feed_dict=feed_dict)        \n",
    "                    new_f_total.append(new_f)\n",
    "                new_f_total = np.array(new_f_total)\n",
    "                use_index= self.use_index\n",
    "                ic = np.zeros((44, self.vali_test-self.train_vali, 4))\n",
    "                for day in range(0, self.vali_test-self.train_vali):\n",
    "                    for fac in range(44):\n",
    "                        for id_idx in range(4):\n",
    "                            rank_ic = stats.spearmanr(self.data.ar_ic[use_index, day, id_idx], new_f_total[:, day, fac])\n",
    "                            ic[fac, day, id_idx] = rank_ic[0]\n",
    "                ic = ic.mean(axis=1)\n",
    "                print(ic)\n",
    "                if not os.path.exists('model-2-'+str(self.save_rank)):\n",
    "                    os.mkdir('model-2-'+str(self.save_rank))\n",
    "                    os.mkdir('data/evaluation-2-'+str(self.save_rank))\n",
    "                pd.DataFrame(ic).to_csv('data/evaluation-2-{}/epoch_evaluation_{}.csv'.format(self.save_rank, epoch))\n",
    "                saver.save(sess, 'model-2-{}/logmodel.ckpt'.format(self.save_rank), global_step=epoch)\n",
    "                \n",
    "                np.random.shuffle(self.rank_day)\n",
    "                loss_all = 0\n",
    "                loss_count = 0\n",
    "                for batch_count in range(batch_num):#batch_num\n",
    "                    print('batch_count:{}'.format(batch_count))\n",
    "                    np.random.shuffle(self.rank_stock)\n",
    "                    for stock_count, stock_idx in enumerate(self.rank_stock):\n",
    "                        price = self.data.price_data[stock_idx, self.rank_day[batch_count*self.batch_size:(batch_count+1)*self.batch_size]]\n",
    "                        feature = self.data.feature_data[stock_idx, self.rank_day[batch_count*self.batch_size:(batch_count+1)*self.batch_size], :]\n",
    "                        label = self.data.ar_trend[stock_idx, self.rank_day[batch_count*self.batch_size:(batch_count+1)*self.batch_size], :]\n",
    "                        stock_name = str(self.data.code_tag[stock_idx])\n",
    "                        for _ in range(6-len(stock_name)):\n",
    "                            stock_name = '0' + stock_name\n",
    "                        if stock_name not in self.data.list_stocks:\n",
    "                            continue\n",
    "                        else:\n",
    "                            embed = np.expand_dims(self.data.embedding[self.data.list_stocks.index(stock_name)], axis=1)\n",
    "                        for epoch_factor in range(30):\n",
    "                            factor_index = random.sample(list(range(44)), 5)\n",
    "                            feed_dict = {self.embedding: embed, self.factor: feature.T, self.factor_index: factor_index, self.label: label}\n",
    "                            _, loss_val = sess.run([self.optimizer, self.cost], feed_dict=feed_dict)\n",
    "                            loss_all += loss_val\n",
    "                            loss_count += 1\n",
    "                    print('avg_loss: {}'.format(loss_all/loss_count))\n",
    "                    loss_all = 0\n",
    "                    loss_count = 0\n",
    "                \n",
    "    def vali(self):\n",
    "        epochs = 50\n",
    "        batch_num = self.train_vali // self.batch_size\n",
    "        saver = tf.train.Saver(max_to_keep=None)\n",
    "        with tf.Session() as sess:\n",
    "            for model_num in range(14, 15):\n",
    "                saver.restore(sess, 'model-1-5/logmodel-{}.ckpt'.format(model_num))\n",
    "                # validation\n",
    "                new_f_total = []\n",
    "                use_index = []\n",
    "                for stock_idx in self.use_index:\n",
    "                    price = self.data.price_data[stock_idx, :self.vali_test]\n",
    "                    feature = self.data.feature_data[stock_idx, :self.vali_test, :]\n",
    "                    label = self.data.ar_trend[stock_idx, :self.vali_test, :]\n",
    "                    stock_name = str(self.data.code_tag[stock_idx])\n",
    "                    for _ in range(6-len(stock_name)):\n",
    "                        stock_name = '0' + stock_name                    \n",
    "                    embed = np.expand_dims(self.data.embedding[self.data.list_stocks.index(stock_name)], axis=1)\n",
    "                    factor_index = random.sample(list(range(44)), 5)\n",
    "                    feed_dict = {self.embedding: embed, self.factor: feature.T, self.factor_index: factor_index, self.label: label}\n",
    "                    new_f= sess.run(self.evaluation, feed_dict=feed_dict)        \n",
    "                    new_f_total.append(new_f)\n",
    "                new_f_total = np.array(new_f_total)\n",
    "                use_index = self.use_index\n",
    "                ic = np.zeros((44, self.vali_test-self.train_vali, 4))\n",
    "                for day in range(self.train_vali, self.vali_test):\n",
    "                    for fac in range(44):\n",
    "                        for id_idx in range(4):\n",
    "                            rank_ic = stats.spearmanr(self.data.ar_ic[use_index, day, id_idx], new_f_total[:, day, fac])\n",
    "                            ic[fac, day-self.train_vali, id_idx] = rank_ic[0]\n",
    "                ic = ic.mean(axis=1)\n",
    "        ic = ic.mean(axis=1)\n",
    "        self.minus_factor = np.where(ic < 0)[0] \n",
    "\n",
    "    def test(self):\n",
    "        epochs = 50\n",
    "        batch_num = self.train_vali // self.batch_size\n",
    "        saver = tf.train.Saver(max_to_keep=None)\n",
    "        with tf.Session() as sess:\n",
    "            for model_num in range(20, 21):\n",
    "                saver.restore(sess, 'model-1-4/logmodel-{}.ckpt'.format(model_num))\n",
    "                # test\n",
    "                new_f_total = []\n",
    "                use_index = []\n",
    "                for stock_idx in self.use_index:\n",
    "                    price = self.data.price_data[stock_idx, :]\n",
    "                    feature = self.data.feature_data[stock_idx, :, :]\n",
    "                    label = self.data.ar_trend[stock_idx, :5, :]\n",
    "                    stock_name = str(self.data.code_tag[stock_idx])\n",
    "                    for _ in range(6-len(stock_name)):\n",
    "                        stock_name = '0' + stock_name                       \n",
    "                    embed = np.expand_dims(self.data.embedding[self.data.list_stocks.index(stock_name)], axis=1)\n",
    "                    factor_index = random.sample(list(range(44)), 5)\n",
    "                    feed_dict = {self.embedding: embed, self.factor: feature.T, self.factor_index: factor_index, self.label: label}\n",
    "                    new_f= sess.run(self.evaluation, feed_dict=feed_dict)        \n",
    "                    new_f_total.append(new_f)\n",
    "                new_f_total = np.array(new_f_total)\n",
    "                use_index = self.use_index\n",
    "                ic = np.zeros((44, self.day_sample-self.vali_test, 4))\n",
    "                for day in range(self.vali_test, self.day_sample):\n",
    "                    for fac in range(44):\n",
    "                        for id_idx in range(4):\n",
    "                            rank_ic = stats.spearmanr(self.data.ar_ic[use_index, day, id_idx], new_f_total[:, day-self.vali_test, fac])\n",
    "                            ic[fac, day-self.vali_test, id_idx] = rank_ic[0]\n",
    "                ic = ic.mean(axis=1)\n",
    "                print(ic)     \n",
    "                print(np.abs(ic).mean())\n",
    "        self.factor_test = new_f_total \n",
    "        self.ic = ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model-1-4/logmodel-20.ckpt\n",
      "[[-0.00736113 -0.01100544 -0.01673548 -0.02973511]\n",
      " [-0.00710425 -0.0109674  -0.01673646 -0.02940153]\n",
      " [-0.00650733 -0.0114733  -0.01714822 -0.02901271]\n",
      " [-0.00725789 -0.01106167 -0.01679869 -0.02962963]\n",
      " [-0.00715966 -0.01079498 -0.01660967 -0.0297128 ]\n",
      " [-0.00733805 -0.0110857  -0.01666332 -0.02939548]\n",
      " [-0.00718852 -0.01096334 -0.01698187 -0.03017095]\n",
      " [-0.0069088  -0.01059303 -0.01573201 -0.02771635]\n",
      " [-0.00767508 -0.01171664 -0.01715455 -0.029422  ]\n",
      " [-0.00733776 -0.01107937 -0.01666719 -0.02954589]\n",
      " [-0.00709149 -0.01111842 -0.01686594 -0.02956678]\n",
      " [-0.00750695 -0.01141749 -0.01713729 -0.02992967]\n",
      " [-0.00700912 -0.01066878 -0.01631177 -0.02898707]\n",
      " [-0.00748071 -0.01152072 -0.01739898 -0.03058096]\n",
      " [-0.00757238 -0.01157514 -0.01751313 -0.0312234 ]\n",
      " [-0.00806036 -0.01294658 -0.01939745 -0.03308506]\n",
      " [-0.01078128 -0.01516858 -0.01847731 -0.02899586]\n",
      " [-0.00559383 -0.00891496 -0.01348178 -0.02235469]\n",
      " [-0.00111774 -0.00381329 -0.00951838 -0.01997894]\n",
      " [-0.00729282 -0.01048487 -0.01739698 -0.03113196]\n",
      " [-0.00537343 -0.00777039 -0.01222255 -0.02295826]\n",
      " [-0.0071758  -0.01077305 -0.01629027 -0.0288548 ]\n",
      " [-0.00728182 -0.01128717 -0.01718594 -0.03045144]\n",
      " [-0.00721836 -0.01111733 -0.01686744 -0.02975767]\n",
      " [-0.00187501 -0.00149202 -0.00508774 -0.0126195 ]\n",
      " [-0.00685119 -0.01026978 -0.01585419 -0.02832131]\n",
      " [-0.00426464 -0.00502139 -0.0082379  -0.01684784]\n",
      " [-0.00730084 -0.01127058 -0.01707668 -0.03000072]\n",
      " [-0.00713686 -0.01096044 -0.01670623 -0.02954091]\n",
      " [-0.00728219 -0.01110601 -0.01683036 -0.02975208]\n",
      " [-0.00300588 -0.00184259 -0.00434959 -0.01494815]\n",
      " [-0.00863661 -0.01346391 -0.01908961 -0.03264927]\n",
      " [-0.00935213 -0.01527287 -0.02224543 -0.03814274]\n",
      " [-0.00733236 -0.01129665 -0.01711167 -0.03022025]\n",
      " [-0.00697631 -0.01070133 -0.01625764 -0.02867105]\n",
      " [-0.00746803 -0.01143517 -0.01714127 -0.02998695]\n",
      " [ 0.00487523  0.00858576  0.00877537  0.00047766]\n",
      " [-0.0065938  -0.01016041 -0.01574146 -0.02788908]\n",
      " [-0.00886085 -0.01361648 -0.02028819 -0.03596473]\n",
      " [-0.00711123 -0.01088704 -0.01654656 -0.02922084]\n",
      " [-0.00738695 -0.01136464 -0.01709309 -0.02997177]\n",
      " [-0.00712276 -0.0109746  -0.01656705 -0.02920143]\n",
      " [ 0.00561597  0.00506848  0.00758427  0.0088292 ]\n",
      " [-0.00343746 -0.00540045 -0.00945755 -0.01847713]]\n",
      "0.0148923593701\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = Model(3)\n",
    "model.data_initial(data)\n",
    "model.data_split()\n",
    "model.factor_network()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.548217744602\n",
      "f1_score 0.576735179396\n",
      "accuracy 0.548264987953\n",
      "f1_score 0.577793232107\n"
     ]
    }
   ],
   "source": [
    "def stock2all(a):\n",
    "    b = a.swapaxes(0, 2)\n",
    "    c = b.reshape(b.shape[0], -1)\n",
    "    d = c.T\n",
    "    return d\n",
    "\n",
    "a = np.array(range(27)).reshape((3, 3, 3))\n",
    "b = stock2all(a)\n",
    "\n",
    "\n",
    "train_x = stock2all(data.feature_data[model.use_index, :model.train_vali, :])\n",
    "train_y = np.argmax(stock2all(model.data.ar_trend[model.use_index, :model.train_vali, :]), axis=1)\n",
    "test_x = stock2all(data.feature_data[model.use_index, model.vali_test:, :])\n",
    "test_y = np.argmax(stock2all(model.data.ar_trend[model.use_index, model.vali_test:, :]), axis=1)\n",
    "\n",
    "lr = LR.LogisticRegression()\n",
    "cf_matrix = np.zeros((2, 2))\n",
    "accuracy = []\n",
    "f1_score = []\n",
    "\n",
    "lr.fit(train_x, train_y)\n",
    "predict_y = lr.predict(test_x)\n",
    "cf_matrix += mt.confusion_matrix(test_y, predict_y)\n",
    "accuracy.append(mt.accuracy_score(test_y, predict_y))\n",
    "f1_score.append(mt.f1_score(test_y, predict_y))\n",
    "\n",
    "print('accuracy', np.array(accuracy).mean())\n",
    "print('f1_score', np.array(f1_score).mean())\n",
    "\n",
    "\n",
    "train_x = np.concatenate([stock2all(model.factor_test[:, :model.train_vali, :]), train_x], axis=1)\n",
    "test_x = np.concatenate([stock2all(model.factor_test[:, model.vali_test:, :]), test_x], axis=1)\n",
    "\n",
    "lr = LR.LogisticRegression()\n",
    "cf_matrix = np.zeros((2, 2))\n",
    "accuracy = []\n",
    "f1_score = []\n",
    "lr.fit(train_x, train_y)\n",
    "predict_y = lr.predict(test_x)\n",
    "cf_matrix += mt.confusion_matrix(test_y, predict_y)\n",
    "accuracy.append(mt.accuracy_score(test_y, predict_y))\n",
    "f1_score.append(mt.f1_score(test_y, predict_y))\n",
    "    \n",
    "print('accuracy', np.array(accuracy).mean())\n",
    "print('f1_score', np.array(f1_score).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
