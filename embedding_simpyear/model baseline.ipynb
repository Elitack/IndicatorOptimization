{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from 'model.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline \n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "#有中文出现的情况，需要u'内容'\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from datum import *\n",
    "import model\n",
    "import imp\n",
    "\n",
    "imp.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Datum()\n",
    "data.data_prepare()\n",
    "data.get_embedding('embedding_all.emb')\n",
    "data.supervised_data_prepare()\n",
    "data.evaluation_prepare()\n",
    "data.label_trend()\n",
    "data.label_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import linear_model as LR\n",
    "from sklearn import metrics as mt\n",
    "import scipy.stats as stats\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from six.moves import xrange\n",
    "\n",
    "import datum\n",
    "\n",
    "data_index = 0\n",
    "len_stock = 3145\n",
    "len_fund  = 2199\n",
    "\n",
    "def stock2all(a):\n",
    "    b = a.swapaxes(0, 2)\n",
    "    c = b.reshape(b.shape[0], -1)\n",
    "    d = c.T\n",
    "    return d\n",
    "\n",
    "a = np.array(range(27)).reshape((3, 3, 3))\n",
    "b = stock2all(a)\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, learning_rate_rank):\n",
    "        self.save_rank = learning_rate_rank\n",
    "        self.learning_rate = 1 / np.power(10, learning_rate_rank)\n",
    "        self.batch_size = 10\n",
    "\n",
    "    def data_initial(self, datum = None):\n",
    "        if datum is not None:\n",
    "            self.data = datum\n",
    "        else:\n",
    "            self.data = datum.Datum()\n",
    "            self.data.data_prepare()\n",
    "            self.data.evaluation_prepare()\n",
    "            self.data.label_trend()\n",
    "            self.data.label_return()\n",
    "        \n",
    "    def data_split(self):\n",
    "        self.day_sample = self.data.price_data.shape[1]\n",
    "        self.stock_sample = self.data.price_data.shape[0]\n",
    "        \n",
    "        self.train_vali = self.day_sample // 2\n",
    "        self.vali_test = self.train_vali + self.day_sample // 4\n",
    "        \n",
    "        self.rank_day = np.array(range(self.train_vali))\n",
    "        self.rank_stock = np.array(range(self.stock_sample))\n",
    "\n",
    "        use_index = []\n",
    "        for stock_idx in range(self.stock_sample):\n",
    "            stock_name = str(self.data.code_tag[stock_idx])\n",
    "            for _ in range(6-len(stock_name)):\n",
    "                stock_name = '0' + stock_name\n",
    "            if stock_name not in self.data.list_stocks:\n",
    "                continue\n",
    "            else:\n",
    "                use_index.append(stock_idx)\n",
    "        self.use_index = np.array(use_index)        \n",
    "        \n",
    "    def factor_network(self):\n",
    "        learning_rate = self.learning_rate\n",
    "        \n",
    "        self.embedding = tf.placeholder(tf.float32, shape=[32, None], name='embedding')\n",
    "        self.factor = tf.placeholder(tf.float32, shape=[44, None], name='factor')\n",
    "        self.factor_index = tf.placeholder(tf.int32, shape=[5], name='factor_index')\n",
    "        self.label = tf.placeholder(tf.int32, shape=[None, 2], name='lable')\n",
    "        \n",
    "        self.weight = tf.get_variable(name='weight', shape=[76, 44], initializer=tf.truncated_normal_initializer(stddev=1.0))\n",
    "        self.bias = tf.get_variable(name='bias', shape=[44], initializer=tf.zeros_initializer)\n",
    "        \n",
    "        self.weight_index = tf.concat([self.factor_index, tf.constant([i for i in range(44, 76)])], axis=0)\n",
    "        self.weight_select = tf.nn.embedding_lookup(self.weight, self.weight_index)\n",
    "        self.factor_select = tf.nn.embedding_lookup(self.factor, self.factor_index)\n",
    "        \n",
    "        self.part_input = tf.concat([self.factor_select, self.embedding], axis=0)\n",
    "        self.part_input = tf.transpose(self.part_input)\n",
    "        self.hidden = tf.matmul(self.part_input, self.weight_select) + self.bias \n",
    "        self.hidden_select = tf.nn.embedding_lookup(tf.transpose(self.hidden), self.factor_index)\n",
    "        \n",
    "        all_feature = tf.concat([tf.transpose(self.factor_select), tf.transpose(self.hidden_select)], 1)\n",
    "        self.pred = tf.contrib.layers.fully_connected(\n",
    "            inputs=all_feature,\n",
    "            num_outputs=2,  # hidden\n",
    "            activation_fn=tf.tanh,\n",
    "            weights_initializer=tf.truncated_normal_initializer(stddev=1.0),\n",
    "            biases_initializer=tf.zeros_initializer()\n",
    "        )\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.pred, labels=self.label))\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.cost)\n",
    "        \n",
    "        self.all_input = tf.concat([self.factor, self.embedding], axis=0)\n",
    "        self.all_input = tf.transpose(self.all_input)\n",
    "        self.evaluation = tf.matmul(self.all_input, self.weight) + self.bias\n",
    "        \n",
    "    def training(self):\n",
    "        epochs = 50\n",
    "        batch_num = self.train_vali // self.batch_size\n",
    "        saver = tf.train.Saver(max_to_keep=None)\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, 'model_initial2/logmodel.ckpt')\n",
    "            for epoch in range(epochs):\n",
    "                print('epoch: {}'.format(epoch))\n",
    "                # validation\n",
    "                new_f_total = []\n",
    "                for stock_idx in range(self.use_index):\n",
    "                    price = self.data.price_data[stock_idx, self.train_vali:self.vali_test]\n",
    "                    feature = self.data.feature_data[stock_idx, self.train_vali:self.vali_test, :]\n",
    "                    label = self.data.ar_trend[stock_idx, self.train_vali:self.vali_test, :]\n",
    "                    stock_name = str(self.data.code_tag[stock_idx])\n",
    "                    embed = np.expand_dims(self.data.embedding[self.data.list_stocks.index(stock_name)], axis=1)\n",
    "                    factor_index = random.sample(list(range(44)), 5)\n",
    "                    feed_dict = {self.embedding: embed, self.factor: feature.T, self.factor_index: factor_index, self.label: label}\n",
    "                    new_f= sess.run(self.evaluation, feed_dict=feed_dict)        \n",
    "                    new_f_total.append(new_f)\n",
    "                new_f_total = np.array(new_f_total)\n",
    "                use_index= self.use_index\n",
    "                ic = np.zeros((44, self.vali_test-self.train_vali, 4))\n",
    "                for day in range(0, self.vali_test-self.train_vali):\n",
    "                    for fac in range(44):\n",
    "                        for id_idx in range(4):\n",
    "                            rank_ic = stats.spearmanr(self.data.ar_ic[use_index, day, id_idx], new_f_total[:, day, fac])\n",
    "                            ic[fac, day, id_idx] = rank_ic[0]\n",
    "                ic = ic.mean(axis=1)\n",
    "                print(ic)\n",
    "                if not os.path.exists('model-2-'+str(self.save_rank)):\n",
    "                    os.mkdir('model-2-'+str(self.save_rank))\n",
    "                    os.mkdir('data/evaluation-2-'+str(self.save_rank))\n",
    "                pd.DataFrame(ic).to_csv('data/evaluation-2-{}/epoch_evaluation_{}.csv'.format(self.save_rank, epoch))\n",
    "                saver.save(sess, 'model-2-{}/logmodel.ckpt'.format(self.save_rank), global_step=epoch)\n",
    "                \n",
    "                np.random.shuffle(self.rank_day)\n",
    "                loss_all = 0\n",
    "                loss_count = 0\n",
    "                for batch_count in range(batch_num):#batch_num\n",
    "                    print('batch_count:{}'.format(batch_count))\n",
    "                    np.random.shuffle(self.rank_stock)\n",
    "                    for stock_count, stock_idx in enumerate(self.rank_stock):\n",
    "                        price = self.data.price_data[stock_idx, self.rank_day[batch_count*self.batch_size:(batch_count+1)*self.batch_size]]\n",
    "                        feature = self.data.feature_data[stock_idx, self.rank_day[batch_count*self.batch_size:(batch_count+1)*self.batch_size], :]\n",
    "                        label = self.data.ar_trend[stock_idx, self.rank_day[batch_count*self.batch_size:(batch_count+1)*self.batch_size], :]\n",
    "                        stock_name = str(self.data.code_tag[stock_idx])\n",
    "                        for _ in range(6-len(stock_name)):\n",
    "                            stock_name = '0' + stock_name\n",
    "                        if stock_name not in self.data.list_stocks:\n",
    "                            continue\n",
    "                        else:\n",
    "                            embed = np.expand_dims(self.data.embedding[self.data.list_stocks.index(stock_name)], axis=1)\n",
    "                        for epoch_factor in range(30):\n",
    "                            factor_index = random.sample(list(range(44)), 5)\n",
    "                            feed_dict = {self.embedding: embed, self.factor: feature.T, self.factor_index: factor_index, self.label: label}\n",
    "                            _, loss_val = sess.run([self.optimizer, self.cost], feed_dict=feed_dict)\n",
    "                            loss_all += loss_val\n",
    "                            loss_count += 1\n",
    "                    print('avg_loss: {}'.format(loss_all/loss_count))\n",
    "                    loss_all = 0\n",
    "                    loss_count = 0\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        epochs = 50\n",
    "        batch_num = self.train_vali // self.batch_size\n",
    "        saver = tf.train.Saver(max_to_keep=None)\n",
    "        with tf.Session() as sess:\n",
    "            for model_num in range(14, 15):\n",
    "                saver.restore(sess, 'baselineModel-5/logmodel-{}.ckpt'.format(model_num))\n",
    "                # test\n",
    "                new_f_total = []\n",
    "                use_index = []\n",
    "                for stock_idx in self.use_index:\n",
    "                    price = self.data.price_data[stock_idx, :]\n",
    "                    feature = self.data.feature_data[stock_idx, :, :]\n",
    "                    label = self.data.ar_trend[stock_idx, :5, :]\n",
    "                    stock_name = str(self.data.code_tag[stock_idx])\n",
    "                    for _ in range(6-len(stock_name)):\n",
    "                        stock_name = '0' + stock_name                       \n",
    "                    embed = np.expand_dims(self.data.embedding[self.data.list_stocks.index(stock_name)], axis=0)\n",
    "                    embed = np.repeat(embed, price.shape[0], 0)\n",
    "                    embed = embed.T                    \n",
    "                    factor_index = random.sample(list(range(44)), 5)\n",
    "                    feed_dict = {self.embedding: embed, self.factor: feature.T, self.factor_index: factor_index, self.label: label}\n",
    "                    new_f= sess.run(self.evaluation, feed_dict=feed_dict)        \n",
    "                    new_f_total.append(new_f)\n",
    "                new_f_total = np.array(new_f_total)\n",
    "                use_index = self.use_index\n",
    "                ic = np.zeros((44, self.day_sample-self.vali_test, 4))\n",
    "                for day in range(self.vali_test, self.day_sample):\n",
    "                    for fac in range(44):\n",
    "                        for id_idx in range(4):\n",
    "                            rank_ic = stats.spearmanr(self.data.ar_ic[use_index, day, id_idx], new_f_total[:, day, fac])\n",
    "                            ic[fac, day-self.vali_test, id_idx] = rank_ic[0]\n",
    "                ic = ic.mean(axis=1)\n",
    "                print(ic)     \n",
    "                print(np.abs(ic).mean())\n",
    "        self.factor_test = new_f_total \n",
    "        self.ic = ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from baselineModel-5/logmodel-14.ckpt\n",
      "[[  2.13792426e-03   2.13009647e-03   8.20993596e-04   7.97427576e-03]\n",
      " [ -4.20355192e-03  -5.75383705e-03  -7.99296683e-03  -4.80902855e-03]\n",
      " [  2.97236534e-03  -2.37738732e-03  -6.38254894e-03  -7.21020853e-03]\n",
      " [ -4.77980524e-03   2.89336733e-03   7.45160966e-03   1.40754564e-02]\n",
      " [ -1.31654402e-03  -4.26482686e-03  -7.22463500e-03  -1.49102564e-02]\n",
      " [ -1.94287668e-03   7.32067623e-03   1.43481473e-02   1.86254881e-02]\n",
      " [  5.63659528e-03   3.44502660e-03   3.03775853e-03  -5.04813771e-03]\n",
      " [ -7.87615553e-03  -6.23258417e-03  -3.79035149e-03   8.30654538e-03]\n",
      " [  7.75679175e-03   1.33586366e-02   1.56873039e-02   1.84717550e-02]\n",
      " [ -3.35472877e-03  -7.00214949e-03  -7.29523113e-03  -9.48015188e-03]\n",
      " [  2.07203506e-03  -1.65077235e-03  -4.30194369e-03  -1.19860231e-02]\n",
      " [  3.71630114e-03   5.62848036e-03   6.44692081e-03  -2.19756143e-04]\n",
      " [  6.60868236e-03   1.99460580e-03   4.50196315e-04  -4.79901392e-03]\n",
      " [  1.37715995e-03   3.19092545e-03   3.90431567e-03  -2.15886193e-03]\n",
      " [ -1.15049947e-03  -2.98219101e-03  -4.68920811e-03   6.22938573e-04]\n",
      " [  4.03138376e-03   5.53891500e-03   6.61531563e-03  -2.20633101e-03]\n",
      " [  2.37701856e-03   5.54882670e-03   8.71644711e-03   1.42624647e-02]\n",
      " [ -1.48391674e-04  -1.88542328e-03  -5.93068212e-03  -1.20422176e-02]\n",
      " [  6.12826441e-03   4.37258192e-04  -3.50298700e-03  -1.24076199e-02]\n",
      " [ -3.56346128e-03  -1.15524771e-03  -1.75413666e-03   3.22036988e-03]\n",
      " [  2.16707948e-03   6.57639267e-03   9.68232409e-03   1.65353653e-02]\n",
      " [ -4.73765963e-03  -5.17618643e-03  -5.78070844e-03  -7.35989879e-03]\n",
      " [ -6.50019113e-03  -1.17973192e-02  -1.43508850e-02  -1.61625899e-02]\n",
      " [  4.88700335e-03  -1.10791528e-04  -2.46228388e-03  -1.01465332e-02]\n",
      " [ -8.15724236e-04   4.57304830e-03   7.31565067e-03   1.35696325e-02]\n",
      " [  3.89761591e-03   5.20629239e-03   8.26096423e-03   1.24777549e-02]\n",
      " [  2.47954057e-04  -1.86527861e-03  -1.19649629e-03   1.77991447e-03]\n",
      " [ -3.22483621e-03  -6.35177350e-03  -9.33341809e-03  -1.11383744e-02]\n",
      " [ -1.53857801e-03  -6.15275586e-03  -1.14962549e-02  -1.67981134e-02]\n",
      " [  3.06269395e-03  -1.32863970e-03  -2.47409596e-03  -7.22011085e-03]\n",
      " [ -3.65078208e-03  -4.01528368e-04  -1.27441087e-03  -5.40114617e-03]\n",
      " [ -5.33432785e-04  -1.17347937e-03  -5.14964955e-03  -6.78664916e-03]\n",
      " [ -5.09603209e-03  -7.56163401e-03  -1.06586296e-02  -1.27699620e-02]\n",
      " [  2.53273292e-03  -2.68275323e-03  -5.00190851e-03  -1.03047165e-02]\n",
      " [  3.61975937e-03   6.01236391e-04   7.26836387e-04  -9.58651240e-05]\n",
      " [ -2.17332722e-04  -2.14459995e-03  -2.82170310e-03   6.54770458e-03]\n",
      " [  8.16891851e-03  -2.21102201e-03  -1.10084863e-02  -2.55972212e-02]\n",
      " [  5.08636607e-03   4.81394162e-03   4.88054312e-03  -2.06743225e-03]\n",
      " [  1.11710973e-02   1.06725031e-02   1.21372288e-02   1.07284757e-02]\n",
      " [  5.85599639e-03   5.71458037e-03   9.70932968e-04  -1.23112652e-04]\n",
      " [  5.54555994e-05  -4.70121889e-03  -8.84412249e-03  -1.58990757e-02]\n",
      " [  1.27965365e-03   4.95355746e-04   1.62978948e-03  -5.73655210e-03]\n",
      " [ -4.87445591e-03  -3.70499966e-03  -4.81075715e-03  -1.69826426e-03]\n",
      " [  2.81991627e-03  -3.01600730e-03  -9.25225693e-03  -1.82342502e-02]]\n",
      "0.00581190925154\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = Model(3)\n",
    "model.data_initial(data)\n",
    "model.data_split()\n",
    "model.factor_network()\n",
    "\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2776, 244, 44)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.factor_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.548217744602\n",
      "f1_score 0.576735179396\n",
      "accuracy 0.543959937639\n",
      "f1_score 0.582563947329\n"
     ]
    }
   ],
   "source": [
    "def stock2all(a):\n",
    "    b = a.swapaxes(0, 2)\n",
    "    c = b.reshape(b.shape[0], -1)\n",
    "    d = c.T\n",
    "    return d\n",
    "\n",
    "a = np.array(range(27)).reshape((3, 3, 3))\n",
    "b = stock2all(a)\n",
    "\n",
    "\n",
    "train_x = stock2all(data.feature_data[model.use_index, :model.train_vali, :])\n",
    "train_y = np.argmax(stock2all(model.data.ar_trend[model.use_index, :model.train_vali, :]), axis=1)\n",
    "test_x = stock2all(data.feature_data[model.use_index, model.vali_test:, :])\n",
    "test_y = np.argmax(stock2all(model.data.ar_trend[model.use_index, model.vali_test:, :]), axis=1)\n",
    "\n",
    "lr = LR.LogisticRegression()\n",
    "cf_matrix = np.zeros((2, 2))\n",
    "accuracy = []\n",
    "f1_score = []\n",
    "\n",
    "lr.fit(train_x, train_y)\n",
    "predict_y = lr.predict(test_x)\n",
    "cf_matrix += mt.confusion_matrix(test_y, predict_y)\n",
    "accuracy.append(mt.accuracy_score(test_y, predict_y))\n",
    "f1_score.append(mt.f1_score(test_y, predict_y))\n",
    "\n",
    "print('accuracy', np.array(accuracy).mean())\n",
    "print('f1_score', np.array(f1_score).mean())\n",
    "\n",
    "\n",
    "train_x = np.concatenate([stock2all(model.factor_test[:, :model.train_vali, :]), train_x], axis=1)\n",
    "test_x = np.concatenate([stock2all(model.factor_test[:, model.vali_test:, :]), test_x], axis=1)\n",
    "\n",
    "lr = LR.LogisticRegression()\n",
    "cf_matrix = np.zeros((2, 2))\n",
    "accuracy = []\n",
    "f1_score = []\n",
    "lr.fit(train_x, train_y)\n",
    "predict_y = lr.predict(test_x)\n",
    "cf_matrix += mt.confusion_matrix(test_y, predict_y)\n",
    "accuracy.append(mt.accuracy_score(test_y, predict_y))\n",
    "f1_score.append(mt.f1_score(test_y, predict_y))\n",
    "    \n",
    "print('accuracy', np.array(accuracy).mean())\n",
    "print('f1_score', np.array(f1_score).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_trend = np.zeros((len(data.code_tag), 244, 2))\n",
    "for count, code in enumerate(data.code_tag):\n",
    "    a_p, a_f = get_data(20160101, 20180000, code)\n",
    "    a_p = a_p[:244+3]\n",
    "    for day in range(244):\n",
    "        if a_p[day+3] > a_p[day]:\n",
    "            ar_trend[count, day, 1] = 1\n",
    "        else:\n",
    "            ar_trend[count, day, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.55588888364\n",
      "f1_score 0.57293263825\n",
      "accuracy 0.552085793925\n",
      "f1_score 0.579052524086\n"
     ]
    }
   ],
   "source": [
    "def stock2all(a):\n",
    "    b = a.swapaxes(0, 2)\n",
    "    c = b.reshape(b.shape[0], -1)\n",
    "    d = c.T\n",
    "    return d\n",
    "\n",
    "a = np.array(range(27)).reshape((3, 3, 3))\n",
    "b = stock2all(a)\n",
    "\n",
    "\n",
    "train_x = stock2all(data.feature_data[model.use_index, :model.train_vali, :])\n",
    "train_y = np.argmax(stock2all(ar_trend[model.use_index, :model.train_vali, :]), axis=1)\n",
    "test_x = stock2all(data.feature_data[model.use_index, model.vali_test:, :])\n",
    "test_y = np.argmax(stock2all(ar_trend[model.use_index, model.vali_test:, :]), axis=1)\n",
    "\n",
    "lr = LR.LogisticRegression(max_iter=5000)\n",
    "cf_matrix = np.zeros((2, 2))\n",
    "accuracy = []\n",
    "f1_score = []\n",
    "\n",
    "lr.fit(train_x, train_y)\n",
    "predict_y = lr.predict(test_x)\n",
    "cf_matrix += mt.confusion_matrix(test_y, predict_y)\n",
    "accuracy.append(mt.accuracy_score(test_y, predict_y))\n",
    "f1_score.append(mt.f1_score(test_y, predict_y))\n",
    "\n",
    "print('accuracy', np.array(accuracy).mean())\n",
    "print('f1_score', np.array(f1_score).mean())\n",
    "\n",
    "\n",
    "train_x = np.concatenate([stock2all(model.factor_test[:, :model.train_vali, :]), train_x], axis=1)\n",
    "test_x = np.concatenate([stock2all(model.factor_test[:, model.vali_test:, :]), test_x], axis=1)\n",
    "\n",
    "lr = LR.LogisticRegression(max_iter=5000)\n",
    "cf_matrix = np.zeros((2, 2))\n",
    "accuracy = []\n",
    "f1_score = []\n",
    "lr.fit(train_x, train_y)\n",
    "predict_y = lr.predict(test_x)\n",
    "cf_matrix += mt.confusion_matrix(test_y, predict_y)\n",
    "accuracy.append(mt.accuracy_score(test_y, predict_y))\n",
    "f1_score.append(mt.f1_score(test_y, predict_y))\n",
    "    \n",
    "print('accuracy', np.array(accuracy).mean())\n",
    "print('f1_score', np.array(f1_score).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_trend = np.zeros((len(data.code_tag), 244, 2))\n",
    "for count, code in enumerate(data.code_tag):\n",
    "    a_p, a_f = get_data(20160101, 20180000, code)\n",
    "    a_p = a_p[:244+10]\n",
    "    for day in range(244):\n",
    "        if a_p[day+10] > a_p[day]:\n",
    "            ar_trend[count, day, 1] = 1\n",
    "        else:\n",
    "            ar_trend[count, day, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.534304577881\n",
      "f1_score 0.596721982152\n",
      "accuracy 0.529993622148\n",
      "f1_score 0.599932642669\n"
     ]
    }
   ],
   "source": [
    "def stock2all(a):\n",
    "    b = a.swapaxes(0, 2)\n",
    "    c = b.reshape(b.shape[0], -1)\n",
    "    d = c.T\n",
    "    return d\n",
    "\n",
    "a = np.array(range(27)).reshape((3, 3, 3))\n",
    "b = stock2all(a)\n",
    "\n",
    "\n",
    "train_x = stock2all(data.feature_data[model.use_index, :model.train_vali, :])\n",
    "train_y = np.argmax(stock2all(ar_trend[model.use_index, :model.train_vali, :]), axis=1)\n",
    "test_x = stock2all(data.feature_data[model.use_index, model.vali_test:, :])\n",
    "test_y = np.argmax(stock2all(ar_trend[model.use_index, model.vali_test:, :]), axis=1)\n",
    "\n",
    "lr = LR.LogisticRegression(max_iter=5000)\n",
    "cf_matrix = np.zeros((2, 2))\n",
    "accuracy = []\n",
    "f1_score = []\n",
    "\n",
    "lr.fit(train_x, train_y)\n",
    "predict_y = lr.predict(test_x)\n",
    "cf_matrix += mt.confusion_matrix(test_y, predict_y)\n",
    "accuracy.append(mt.accuracy_score(test_y, predict_y))\n",
    "f1_score.append(mt.f1_score(test_y, predict_y))\n",
    "\n",
    "print('accuracy', np.array(accuracy).mean())\n",
    "print('f1_score', np.array(f1_score).mean())\n",
    "\n",
    "\n",
    "train_x = np.concatenate([stock2all(model.factor_test[:, :model.train_vali, :]), train_x], axis=1)\n",
    "test_x = np.concatenate([stock2all(model.factor_test[:, model.vali_test:, :]), test_x], axis=1)\n",
    "\n",
    "lr = LR.LogisticRegression(max_iter=5000)\n",
    "cf_matrix = np.zeros((2, 2))\n",
    "accuracy = []\n",
    "f1_score = []\n",
    "lr.fit(train_x, train_y)\n",
    "predict_y = lr.predict(test_x)\n",
    "cf_matrix += mt.confusion_matrix(test_y, predict_y)\n",
    "accuracy.append(mt.accuracy_score(test_y, predict_y))\n",
    "f1_score.append(mt.f1_score(test_y, predict_y))\n",
    "    \n",
    "print('accuracy', np.array(accuracy).mean())\n",
    "print('f1_score', np.array(f1_score).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.abs(model.final_ic)).to_csv('simple_ic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('baselineModel-5'):\n",
    "    piece_name = filename.split('.')\n",
    "    if piece_name[0] == 'logmodel':\n",
    "        split1 = piece_name[1].split('-')\n",
    "        new_name = '.'.join([piece_name[0]+'-'+split1[1], split1[0], piece_name[2]])\n",
    "        os.rename('baselineModel-5/'+filename, 'baselineModel-5/'+new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('baselineModel-4'):\n",
    "    piece_name = filename.split('.')\n",
    "    if piece_name[0] == 'logmodel':\n",
    "        split1 = piece_name[1].split('-')\n",
    "        new_name = '.'.join([piece_name[0]+'-'+split1[1], split1[0], piece_name[2]])\n",
    "        os.rename('baselineModel-4/'+filename, 'baselineModel-4/'+new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('model-1-4'):\n",
    "    piece_name = filename.split('.')\n",
    "    if piece_name[0] == 'logmodel':\n",
    "        split1 = piece_name[1].split('-')\n",
    "        new_name = '.'.join([piece_name[0]+'-'+split1[1], split1[0], piece_name[2]])\n",
    "        os.rename('model-1-4/'+filename, 'model-1-4/'+new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010319475452220507"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(model.final_ic)[:, 3].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
